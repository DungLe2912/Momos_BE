/* eslint-disable indent */
/* eslint-disable consistent-return */
/* eslint-disable no-loop-func */
/* eslint-disable no-plusplus */
/* eslint-disable function-paren-newline */
/* eslint-disable implicit-arrow-linebreak */
/* eslint-disable no-undef */
/* eslint-disable no-restricted-syntax */
/* eslint-disable class-methods-use-this */
/* eslint-disable no-await-in-loop */
import puppeteer from "puppeteer";
import Bull from "bull";
import Redis from "ioredis";
import { Op } from "sequelize";

import { logger } from "../middlewares/logger.middleware.js";
import db from "../database/index.js";
import { browserManager } from "../utils/browser.js";

const BATCH_SIZE = 100; // Sá»‘ lÆ°á»£ng URLs xá»­ lÃ½ trong 1 batch
const CONCURRENT_JOBS = 5; // Sá»‘ lÆ°á»£ng jobs cháº¡y Ä‘á»“ng thá»i
const CACHE_TTL = 3600; // Cache thá»i gian 1 giá»
const MAX_RETRIES = 3;
const BROWSER_POOL_SIZE = 3; // Sá»‘ lÆ°á»£ng browser instances trong pool

class ScraperService {
  constructor() {
    this.redis = new Redis({
      host: process.env.REDIS_HOST || "redis",
      port: parseInt(process.env.REDIS_PORT || "6379"),
      password: process.env.REDIS_PASSWORD,
      retryStrategy: (times) => {
        const delay = Math.min(times * 50, 2000);
        return delay;
      },
      maxRetriesPerRequest: 3,
    });

    this.scrapeQueue = new Bull("scrapeQueue", {
      redis: {
        host: process.env.REDIS_HOST,
        port: process.env.REDIS_PORT,
        password: process.env.REDIS_PASSWORD,
      },
      limiter: {
        max: 100,
        duration: 10000,
      },
      defaultJobOptions: {
        removeOnComplete: false, // Do not remove completed jobs
        removeOnFail: false, // Do not remove failed jobs
        attempts: 3,
        backoff: {
          type: "exponential",
          delay: 2000,
        },
      },
    });

    // ThÃªm event listeners Ä‘á»ƒ theo dÃµi job states
    this.scrapeQueue.on("active", async (job) => {
      console.log(`Job ${job.id} active`);
      // this.updateJobStats(job, "completed");
      await this.redis.set(`job:${job.id}`, "active", "EX", CACHE_TTL);
    });

    this.scrapeQueue.on("completed", async (job) => {
      console.log(`Job ${job.id} completed`);
      // this.updateJobStats(job, "completed");
      await this.redis.set(`job:${job.id}`, "completed", "EX", CACHE_TTL);
    });

    this.scrapeQueue.on("failed", async (job) => {
      console.log(`Job ${job.id} failed`);
      // this.updateJobStats(job, "failed");
      await this.redis.set(`job:${job.id}`, "failed", "EX", CACHE_TTL);
    });

    // Khá»Ÿi táº¡o Redis key Ä‘á»ƒ lÆ°u job stats
    this.jobStatsKey = "scrapeQueue:stats";

    // Process queue vá»›i concurrency control
    this.scrapeQueue.process(CONCURRENT_JOBS, this.processJob.bind(this));

    // Cleanup khi shutdown
    process.on("SIGTERM", this.cleanup.bind(this));
    process.on("SIGINT", this.cleanup.bind(this));
  }

  // Method Ä‘á»ƒ update job stats trong Redis
  async updateJobStats(job, status) {
    try {
      const accountId = job.data.accountId;
      const statsKey = `${this.jobStatsKey}:${accountId}`;

      await this.redis.hincrby(statsKey, status, 1);
      await this.redis.hincrby(statsKey, "total", 1);

      // Set expiration cho stats (vÃ­ dá»¥: 30 ngÃ y)
      await this.redis.expire(statsKey, 30 * 24 * 3600);
    } catch (error) {
      logger.error("Failed to update job stats:", error);
    }
  }

  async processScrapedData(urls, media, accountId) {
    try {
      // Validate input
      if (!Array.isArray(urls) || !Array.isArray(media) || !accountId) {
        throw new Error(
          "Invalid input: urls and media must be arrays, and accountId is required"
        );
      }

      const mediaRecords = [];

      // Process tá»«ng URL vÃ  tÃ¡ch images/videos thÃ nh cÃ¡c records riÃªng
      urls.forEach((sourceUrl, index) => {
        const currentMedia = media[index];

        // Process images
        if (Array.isArray(currentMedia.images)) {
          currentMedia.images.forEach((imageUrl) => {
            mediaRecords.push({
              url: sourceUrl,
              type: "image",
              src: imageUrl,
              accountId,
            });
          });
        }

        // Process videos
        if (Array.isArray(currentMedia.videos)) {
          currentMedia.videos.forEach((videoUrl) => {
            mediaRecords.push({
              url: sourceUrl,
              type: "video",
              src: videoUrl,
              accountId,
            });
          });
        }
      });

      // Validate URLs vÃ  filter out invalid ones
      const validMediaRecords = mediaRecords.filter((record) => {
        try {
          new URL(record.src);
          return true;
        } catch (err) {
          logger.warn(
            `Invalid media URL: ${record.src} from source: ${record.url}`
          );
          return false;
        }
      });

      // Batch insert vá»›i chunk size Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i database
      const CHUNK_SIZE = 1000;
      const results = [];

      for (let i = 0; i < validMediaRecords.length; i += CHUNK_SIZE) {
        const chunk = validMediaRecords.slice(i, i + CHUNK_SIZE);

        // Bulk create vá»›i handling duplicates
        const created = await db.models.media.bulkCreate(chunk, {
          updateOnDuplicate: ["type", "src", "updatedAt"],
          returning: true,
        });

        results.push(...created);
      }

      // Cache results grouped by source URL
      const groupedResults = results.reduce((acc, record) => {
        if (!acc[record.url]) {
          acc[record.url] = {
            images: [],
            videos: [],
          };
        }

        if (record.type === "image") {
          acc[record.url].images.push(record.src);
        } else {
          acc[record.url].videos.push(record.src);
        }

        return acc;
      }, {});

      // Update cache
      await Promise.all(
        Object.entries(groupedResults).map(([url, mediaData]) =>
          this.redis.set(
            `media:${accountId}:${url}`,
            JSON.stringify(mediaData),
            "EX",
            3600
          )
        )
      );

      // Return summary
      return {
        processed: {
          urls: new Set(results.map((r) => r.url)).size,
          total: results.length,
          images: results.filter((r) => r.type === "image").length,
          videos: results.filter((r) => r.type === "video").length,
        },
        results: groupedResults,
      };
    } catch (error) {
      logger.error("Failed to process scraped data:", error);
      throw error;
    }
  }

  async scrapeImageAndVideoURLs(urls, browser) {
    console.log(
      "ðŸš€ ~ ScraperService ~ scrapeImageAndVideoURLs ~ browser:",
      browser
    );
    const results = [];
    const page = await browser.newPage();
    console.log("ðŸš€ ~ ScraperService ~ scrapeImageAndVideoURLs ~ page:", page);

    try {
      await page.setUserAgent(
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
      );

      // Optimize memory usage
      await page.setCacheEnabled(false);
      await page.setRequestInterception(true);

      page.on("request", (request) => {
        if (["image", "stylesheet", "font"].includes(request.resourceType())) {
          request.abort();
        } else {
          request.continue();
        }
      });

      for (const url of urls) {
        try {
          await page.goto(url, {
            waitUntil: "networkidle0",
            timeout: 60000,
          });

          const media = await page.evaluate(() => {
            const images = new Set();
            const videos = new Set();

            // Collect images
            document
              .querySelectorAll("img[src]")
              .forEach((img) => images.add(img.src));

            // Collect videos
            document
              .querySelectorAll("video source, video[src]")
              .forEach((video) => videos.add(video.src || video.currentSrc));

            return {
              images: Array.from(images).filter((src) =>
                src.startsWith("http")
              ),
              videos: Array.from(videos).filter((src) =>
                src.startsWith("http")
              ),
            };
          });

          results.push(media);
        } catch (error) {
          logger.error(`Error scraping ${url}: ${error.message}`);
          results.push({ images: [], videos: [] });
        }
      }

      return results;
    } finally {
      await page.close();
    }
  }

  async addScrapingTask(urls, accountId) {
    // Normalize to array
    const urlsArray = Array.isArray(urls) ? urls : [urls];

    // Check if the URLs were recently processed to prevent duplicate requests
    const recentUrls = await Promise.all(
      urlsArray.map((url) => this.redis.get(`recent:${accountId}:${url}`))
    );

    if (urlsArray.filter((_, index) => !recentUrls[index]).length === 0) {
      return {
        status: "skipped",
        message: "URLs were recently processed, skipping these requests.",
      };
    }

    // Check cache first
    const cachedResults = await Promise.all(
      urlsArray.map((url) => this.redis.get(`media:${accountId}:${url}`))
    );

    const uncachedUrls = urlsArray.filter((_, index) => !cachedResults[index]);

    if (uncachedUrls.length === 0) {
      return cachedResults.map((result) => JSON.parse(result));
    }

    // Split URLs into batches
    const batches = [];
    for (let i = 0; i < uncachedUrls.length; i += BATCH_SIZE) {
      batches.push(uncachedUrls.slice(i, i + BATCH_SIZE));
    }

    // Add batches to queue
    const jobs = await Promise.all(
      batches.map((batch) =>
        this.scrapeQueue.add(
          {
            urls: batch,
            accountId,
          },
          {
            attempts: MAX_RETRIES,
            backoff: {
              type: "exponential",
              delay: 2000,
            },
            removeOnComplete: true,
            removeOnFail: false,
          }
        )
      )
    );

    // Cache the processed URLs to prevent resubmission in a short time
    await Promise.all(
      uncachedUrls.map((url) =>
        this.redis.setex(`recent:${accountId}:${url}`, 60, "processed")
      )
    );

    return {
      status: "queued",
      jobIds: jobs.map((job) => job.id),
      message: `Added ${uncachedUrls.length} URLs to processing queue in ${batches.length} batches`,
      cachedResults: cachedResults.filter(Boolean).map((r) => JSON.parse(r)),
    };
  }

  async processJob(job) {
    const { urls, accountId } = job.data;
    let currentAttempt = 0;

    while (currentAttempt < MAX_RETRIES) {
      try {
        const browser = await browserManager.getBrowser();
        const media = await this.scrapeImageAndVideoURLs(urls, browser);
        return await this.processScrapedData(urls, media, accountId);
      } catch (error) {
        currentAttempt++;
        logger.error(`Attempt ${currentAttempt} failed: ${error.message}`);
        if (currentAttempt === MAX_RETRIES) throw error;
        await new Promise((resolve) => {
          setTimeout(resolve, 2000 * currentAttempt);
        });
      }
    }
  }

  async getScrapingResults({ jobIds, urls, page, limit, status, accountId }) {
    try {
      // Base where condition vá»›i accountId
      const where = { accountId };
      const targetUrls = new Set();

      // 1. Náº¿u cÃ³ jobIds, láº¥y URLs tá»« cÃ¡c jobs trÆ°á»›c
      if (jobIds && jobIds.length > 0) {
        const jobs = await Promise.all(
          jobIds.map((id) => this.scrapeQueue.getJob(id))
        );

        // Filter jobs theo status náº¿u cÃ³
        const filteredJobs = jobs.filter(
          (job) =>
            job &&
            job.data.accountId === accountId &&
            (!status || job.status === status)
        );

        // Collect táº¥t cáº£ URLs tá»« cÃ¡c jobs phÃ¹ há»£p
        filteredJobs.forEach((job) => {
          job.data.urls.forEach((url) => targetUrls.add(url));
        });
      }

      // 2. ThÃªm URLs Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh trá»±c tiáº¿p
      if (urls && urls.length > 0) {
        urls.forEach((url) => targetUrls.add(url));
      }

      // 3. Náº¿u cÃ³ URLs Ä‘á»ƒ filter
      if (targetUrls.size > 0) {
        where.url = {
          [Op.in]: Array.from(targetUrls),
        };
      }

      // 4. Láº¥y káº¿t quáº£ tá»« database vá»›i pagination
      const results = await db.models.media.findAndCountAll({
        where,
        limit,
        offset: (page - 1) * limit,
        order: [["createdAt", "DESC"]],
        attributes: ["id", "url", "images", "videos", "createdAt", "updatedAt"],
      });

      // 5. Kiá»ƒm tra cache cho cÃ¡c URLs chÆ°a cÃ³ trong database
      const foundUrls = new Set(results.rows.map((r) => r.url));
      const missingUrls = Array.from(targetUrls).filter(
        (url) => !foundUrls.has(url)
      );

      if (missingUrls.length > 0) {
        const cachedResults = await Promise.all(
          missingUrls.map(async (url) => {
            const cached = await this.redis.get(`media:${accountId}:${url}`);
            if (!cached) return null;

            try {
              const parsed = JSON.parse(cached);
              return {
                url,
                ...parsed,
                fromCache: true,
                createdAt: new Date(),
                updatedAt: new Date(),
              };
            } catch (e) {
              logger.error(`Failed to parse cached result for ${url}:`, e);
              return null;
            }
          })
        );

        // Add valid cached results
        const validCachedResults = cachedResults.filter(Boolean);
        results.rows = [...results.rows, ...validCachedResults];
        results.count += validCachedResults.length;
      }

      // 6. ThÃªm thÃ´ng tin job status cho má»—i result
      const enrichedResults = await Promise.all(
        results.rows.map(async (result) => {
          if (result.fromCache) return result;

          // TÃ¬m job tÆ°Æ¡ng á»©ng vá»›i URL nÃ y
          const relatedJob = jobIds
            ? (
                await Promise.all(
                  jobIds.map((id) => this.scrapeQueue.getJob(id))
                )
              ).find((job) => job && job.data.urls.includes(result.url))
            : null;

          return {
            ...result.toJSON(),
            jobInfo: relatedJob
              ? {
                  jobId: relatedJob.id,
                  status: await relatedJob.getState(),
                  processedAt: relatedJob.processedOn,
                  finishedAt: relatedJob.finishedOn,
                }
              : undefined,
          };
        })
      );

      // 7. Prepare response vá»›i additional metadata
      return {
        data: enrichedResults,
        pagination: {
          total: results.count,
          page,
          totalPages: Math.ceil(results.count / limit),
        },
        summary: {
          totalUrls: targetUrls.size,
          foundInDb: results.rows.filter((r) => !r.fromCache).length,
          foundInCache: results.rows.filter((r) => r.fromCache).length,
          missing: targetUrls.size - results.rows.length,
        },
        jobsInfo: jobIds
          ? await this.getJobsBasicInfo(jobIds, accountId)
          : undefined,
      };
    } catch (error) {
      logger.error("Failed to get scraping results:", error);
      throw error;
    }
  }

  // Helper method Ä‘á»ƒ láº¥y basic job info
  async getJobsBasicInfo(jobIds, accountId) {
    const jobs = await Promise.all(
      jobIds.map(async (id) => {
        const job = await this.scrapeQueue.getJob(id);
        if (!job || job.data.accountId !== accountId) return null;

        return {
          id: job.id,
          status: await job.getState(),
          // eslint-disable-next-line no-underscore-dangle
          progress: job._progress,
          urls: job.data.urls.length,
          createdAt: job.timestamp,
          processedAt: job.processedOn,
          finishedAt: job.finishedOn,
          error: job.failedReason,
        };
      })
    );

    return jobs.filter(Boolean);
  }

  async getJobsStatus(jobIds, accountId) {
    try {
      // Retrieve job statuses from Redis
      const jobs = await Promise.all(
        jobIds.map(async (id) => {
          const status = await this.redis.get(`job:${id}`);
          return { id, status };
        })
      );

      return {
        jobs,
      };
    } catch (error) {
      logger.error("Failed to get jobs status:", error);
      throw error;
    }
  }

  async getScrapingStats(accountId) {
    try {
      // Láº¥y thá»‘ng kÃª tá»« database
      const dbStats = await db.models.media.findOne({
        where: { accountId },
        attributes: [
          [db.fn("COUNT", db.col("id")), "totalUrls"],
          [
            db.fn(
              "SUM",
              db.cast(db.fn("jsonb_array_length", db.col("images")), "INTEGER")
            ),
            "totalImages",
          ],
          [
            db.fn(
              "SUM",
              db.cast(db.fn("jsonb_array_length", db.col("videos")), "INTEGER")
            ),
            "totalVideos",
          ],
        ],
        raw: true,
      });

      // 1. Láº¥y real-time queue stats
      const currentCounts = await this.scrapeQueue.getJobCounts();

      // 2. Láº¥y accumulated stats tá»« Redis
      const statsKey = `${this.jobStatsKey}:${accountId}`;
      const accumulatedStats = (await this.redis.hgetall(statsKey)) || {
        completed: "0",
        failed: "0",
        total: "0",
      };

      // 3. Láº¥y active jobs cho account
      const activeJobs = await this.scrapeQueue.getActive();
      const accountActiveJobs = activeJobs.filter(
        (job) => job.data.accountId === accountId
      );

      // 4. Láº¥y waiting jobs cho account
      const waitingJobs = await this.scrapeQueue.getWaiting();
      const accountWaitingJobs = waitingJobs.filter(
        (job) => job.data.accountId === accountId
      );

      // 5. TÃ­nh performance metrics
      const completedJobs = parseInt(accumulatedStats.completed || 0);
      const failedJobs = parseInt(accumulatedStats.failed || 0);
      const totalProcessed = completedJobs + failedJobs;

      return {
        mediaStats: {
          totalUrls: parseInt(dbStats?.totalUrls, 10) || 0,
          totalImages: parseInt(dbStats?.totalImages, 10) || 0,
          totalVideos: parseInt(dbStats?.totalVideos, 10) || 0,
        },
        queueStats: {
          waiting: accountWaitingJobs.length,
          active: accountActiveJobs.length,
          completed: completedJobs,
          failed: failedJobs,
          total: parseInt(accumulatedStats.total || 0),
        },
        performance: {
          successRate:
            totalProcessed > 0 ? (completedJobs / totalProcessed) * 100 : 0,
          totalProcessed,
          currentlyProcessing: accountActiveJobs.length,
        },
        currentQueueState: {
          ...currentCounts,
          timestamp: new Date(),
        },
      };
    } catch (error) {
      logger.error("Failed to get scraping stats:", error);
      throw error;
    }
  }

  async cleanup() {
    logger.info("Cleaning up resources...");

    // await this.redis.flushall();

    await Promise.all([
      ...this.browserPool.map((browser) => browser.close()),
      this.redis.quit(),
      this.scrapeQueue.close(),
    ]);

    logger.info("Cleanup completed!");
  }
}

export default new ScraperService();
